
# ğŸš€ **Kaggle Spaceship Titanic - Machine Learning Project** ğŸŒŒ



## ğŸ“ **Project Overview**

Welcome to my **Kaggle Spaceship Titanic** competition project! ğŸ¯ The challenge is to predict whether a passenger was transported to another dimension during the **Spaceship Titanic** disaster.

Explore the full project notebook with code, explanations, and visualizations here:  
ğŸ‘‰ [**Kaggle Notebook**](https://www.kaggle.com/code/susanta21/kaggle-spaceship-titanic)

## ğŸš¢ **Problem Statement**

The goal is to build a machine learning model to predict the `Transported` status of passengers based on the following features:

- **PassengerId** ğŸ†”: Unique identifier for each passenger.
- **HomePlanet** ğŸª: The planet the passenger originated from.
- **CryoSleep** ğŸ’¤: Whether the passenger was in suspended animation.
- **Cabin** ğŸ›ï¸: Cabin details of the passenger.
- **Destination** ğŸŒ : Destination planet for the passenger.
- **Age** ğŸ‚: Age of the passenger.
- **VIP** ğŸ’: VIP service status.
- **RoomService, FoodCourt, ShoppingMall, Spa, VRDeck** ğŸ›ï¸: Expenditure in these facilities.
- **Name** ğŸ‘¤: Passenger's name.
- **Transported** âœ…/âŒ: The target variable indicating if the passenger was transported.

## ğŸ” **Approach**

### 1. **Data Exploration and Preprocessing** ğŸ§¹
   - **Handling Missing Values**: Cleaned up missing data to ensure consistent model input.
   - **Feature Engineering** ğŸ› ï¸: Extracted new features from existing ones (like deck and cabin number from `Cabin`).
   - **Normalization** ğŸ“Š: Scaled numerical features for better performance.

### 2. **Model Selection** ğŸ¤–
   Experimented with several machine learning models to find the best one:
   - **Logistic Regression**
   - **Decision Trees**
   - **Random Forest**
   - **Gradient Boosting**
   - **XGBoost**

### 3. **Model Tuning & Evaluation** ğŸ”§
   - **Hyperparameter Tuning**: Fine-tuned models for optimal accuracy.
   - **Evaluation Metrics**: Used accuracy, F1 score, and ROC-AUC to assess model performance.

### 4. **Final Model & Prediction** ğŸ‰
   The best-performing model was selected for generating final predictions, achieving significant accuracy on the test data.

## ğŸ’» **Technologies Used**

- **Python** ğŸ: The core programming language.
- **Pandas** ğŸ“‘: For data manipulation and preprocessing.
- **Matplotlib & Seaborn** ğŸ“Š: For data visualization.
- **Scikit-learn** ğŸ§ : For building and evaluating machine learning models.
- **Kaggle Notebooks** ğŸ“’: Used to develop and run the entire project.

## ğŸ¯ **Results**

The final model achieved an impressive accuracy score of `81` (replace with your actual score) on the test data. ğŸš€



## ğŸ› ï¸ **How to Use**

To reproduce the results from this project, follow these steps:

1. **Clone this repository**:  
   ```bash
   git clone https://github.com/your-repo/spaceship-titanic.git
   ```

2. **Install dependencies**:  
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the notebook**:  
   ```bash
   jupyter notebook kaggle-spaceship-titanic.ipynb
   ```

## ğŸ“Œ **Kaggle Notebook**

For the full walkthrough of the solution, including code, insights, and visualizations, visit the Kaggle notebook here:  
ğŸ‘‰ [**View Kaggle Notebook**](https://www.kaggle.com/code/susanta21/kaggle-spaceship-titanic)

## ğŸ† **Conclusion**

The **Spaceship Titanic** competition provided an exciting opportunity to experiment with machine learning models for binary classification. The notebook demonstrates feature engineering, model selection, and performance tuning to achieve optimal results.

Thank you for checking out the project! ğŸŒŸ Feel free to reach out if you have any questions or suggestions.
